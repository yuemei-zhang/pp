

select a.*,b.*,c.*
from a
lateral view json_tuple (a.res_param,'cacl_amount','cas_float','casl') b as cacl_amount,cas_float,
casl
lateral view json_tuple (a.res_param,'al','bl','cl') c as al,bl,cl;

get_json_object(inputparam,'$.user_id') as user_id
substr(apprequestied,locate('R',apprequestied)) as borrow_nid


=OFFSET(监控报表样版!$G4,0,(COLUMN(W3)-1)*8+$B$7)
=INDEX(爬虫成功率统计!$A:$I,ROW(),COLUMN())
=INDEX(爬虫成功率统计!$A:$I,ROW(),$M$14+3)
=IF(M3=1,L3,IF(M3=2,L4,IF(M3=3,L5,IF(M3=4,L6,IF(M3=5,L7,IF(M3=6,L8,FALSE))))))
=SUMIFS($D:$D,$A:$A,$AE5,$B:$B,$M$4,$C:$C,AF$4)

近3个月主叫通话时长30秒内的总次数	
近3个月内主叫城市数	
近3个月内主叫天数占比	
近3个月被叫（运营商类）的总次数	
近1个月内早上主叫通话时长	
7天内短信总条数	
近三个月内同一号码接收短信最大条数	
近一个月内中午通话时长占比	
借贷当月账单总-套餐金额	
近三个月内发送短信号码（手机类）数占比	
借贷前第4个月账单总金额	



--分箱


proc sql noprint;
proc contents data=base_old_1 out =conts;
run;
proc sql  noprint;
select compress(name) into : InputVar_List separated by ' ' from conts where 
name not in ('borrow_nid','channel_category','first_flag_30','max_flag_10');
quit;


/*data1单期*/
%let Input_DSN=base_old_1;							 	
%let Target =first_flag_30; 										
%let Output_DSN_Detail =Output_DSN_Detail;				
%let Output_DSN_Smry =Output_DSN_Smry;				
%let Output_DSN_Sel_Detail=Output_DSN_Sel_Detail;
%let Output_DSN_Sel_Smry =var_old_first;			
%let output_missing_var=output_missing_var;


	

%include "P:\policy_kyd\zhangyuemei\手机贷\sas\zhuhui_continuous_binning_chimerge.sas";

%Num_Var_Binning
		(	Input_DSN=&Input_DSN, 							 	/*input data set name*/
			Target = &target , 										/*target binary variables*/
			InputVar_List =&InputVar_List, 							/*input continuous variables list, separated by blanks*/			
			Max_Missing_Portion =1, 			/* how big of the  portiton of missing obs out of total obs is allowed; 0~1*/
			Initial_Bin_Num = 50,						/*Inital bin number, recommeded:100 or 50 or 20 or 16 or 8*/
			Inital_Bin_Method = 2, 					/* Equal Width(Bucket) or Equal Height(quantitle).  1: Equal Width; 2: Equal Height*/
			Max_Bin_Num = 15, 							/*the maximum number of bins are allowed to form, 10~15*/
			ChiSquare_Sign_Level =0.1 ,				/* signicance level for the independence chi-square test */
			Info_Value_Threshold =0.01, 				/*  information threshould for variable selection */
			Output_DSN_Detail =&Output_DSN_Detail,					/*Output dataset name, detailed info   */
			Output_DSN_Smry =&Output_DSN_Smry,					/*Output dataset name, summary info  */
			Output_DSN_Sel_Detail=&Output_DSN_Sel_Detail,
			Output_DSN_Sel_Smry =&Output_DSN_Sel_Smry,				/*Output dataset name, summary info */
			output_missing_var=&output_missing_var					/*Output missing_var */
		);
--转置
proc transpose data=pa_classifier_user_4 out=pa_classifier_user_5;
by userid target ;/*对每个by组求转置*/
id classifier_user;/*转置后数据集的变量名*/
var cnt ;/*要转置的变量*/
run;



--分位数

data tb;
set hive.tmp_zym_tmp_zym_big_zr_auth;
where minus>0 and minus<10;
keep user_id auth_time minus add_time;
run;

proc sort data=tb out=tb_1 nodupkey;by user_id minus;run;



proc univariate data =tb_1;
var minus;
output out=outtable p1 = p1 
    p5 = p5 
    p10 = p10
;
run;


/*ARRAY定义数组,在数据集结构变化的情况下使用，_numeric_定义数值型变量*/ 
/*ARRA常与DO循环语句连用，目的：缺失值填充；创建累加器；横向比较变量；纵向横向综合应用*/
/*array <变量名> _numeric_;定义数值型变量*/
/*array <变量名> _character_;定义字符型变量*/

data missing;
 input x y$ z$ m;
 cards;
 . . . 1
 2 . 3 .
 ;
run;

data result;/*缺失值填充*/
 set missing;
 array char _character_;/*声明所有字符变量*/
 array numr _numeric_; /*声明所有数值变量*/
 do over char;/*循环所有数组char对应的变量*/
 if char="" then char="null";/*所有字符变量填充null*/
 end;
 do over numr;
 if numr=. then numr=0;
/* if missing(numr) then numr=0;*/ 
 end;
run;

apprequestid_1=scan(apprequestid,1,'_');

apprequestid_2=substr(apprequestid,find(apprequestid,'_')+1);

--SCORE
import pandas as pd
import numpy as np

%matplotlib inline
import matplotlib.pyplot as plt
import matplotlib

import sc_function as sc
import MyFuncs as Foo

raw_data = pd.read_csv("score_data")
star_data = sc.clean_name(raw_data)
star_data = star_data.drop(['Unnamed: 0'], axis='columns')
star_data.set_index("id_no", inplace=True)

model_cols = ['x0061', 'u1165', 'u1001', 'y0014', 's0322', 'max_late_days']
train_data = data[model_cols]
train_data  = train_data.loc[ ((train_data.max_late_days == 0) |  (train_data.max_late_days > 30))]
train_data = train_data.astype('float')
train_data['y'] = train_data.max_late_days >=30

train_data



feature processing
optedBinds = Foo.getOptimalBins(inDs =train_data, vars = ['x0061', 'u1165', 'u1001', 'y0014', 's0322', 'bdgbdt', 'td_1m_ph_cnt',
       'td_7days_tot_cnt_p2p', 'td_3m_tot_cnt_insure', 'td_3m_id_cnt'], targVar = "y", drop_na = False)

## cut bin
iv_map = Foo.collectWoeTabs(train_data, binDict=optedBinds, targVar="y")
bin_data = pd.get_dummies(train_data.filter(like="bin", axis=1), dummy_na=True)
train_bin_data = train_data[["y"]].join(bin_data, how='inner')

## display
for key,value in iv_map.items():
    display(value.style.bar(subset=["Woe"], align='mid',color=['#5F9EA0','#A52A2A']))



from sklearn.model_selection import GridSearchCV, train_test_split

y = train_bin_data.y
X = train_bin_data.drop("y", axis=1)

trainData, testData = train_test_split(train_bin_data,test_size=0.2)
y_train = trainData.y
X_train = trainData.drop("y", axis=1)

y_test = testData.y
X_test = testData.drop("y", axis=1)



model selection

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier

lr = LogisticRegression(penalty='l2',n_jobs=30, fit_intercept=True, solver='newton-cg')
gbdt = GradientBoostingClassifier(random_state=10)
rf = RandomForestClassifier(n_estimators = 50, criterion = 'gini')
adaboost = AdaBoostClassifier(n_estimators = 200, learning_rate = 1.0)

models = [lr, gbdt, rf, adaboost]

import time

def model_selection(models, X_train, y_train, X_test, y_test):
    for model in models:
        s = time.perf_counter()   
        model.fit(X_train, y_train)
        e = time.perf_counter()
        print('model: {}, time:{} '.format(str(model), (e-s)))
        sc.KS_Graph(y_train, model.predict_proba(X_train)[:,1])
        sc.KS_Graph(y_test, model.predict_proba(X_test)[:,1])


model_selection(models, X_train, y_train, X_test, y_test)



不进行分bin处理


cols = model_cols + ['y']
sample = train_data[cols].fillna(-1)

y2 = sample.y
X2 = sample

trainData2, testData2 = train_test_split(sample,test_size=0.2)
y_train2 = trainData2.y 
X_train2 = trainData2.drop(['y','max_late_days'], axis=1)

y_test2 = testData2.y
X_test2 = testData2.drop(['y','max_late_days'], axis=1)

model_selection(models, X_train2, y_train2, X_test2, y_test2)


LightGBM


import lightgbm as lgb

gbm = lgb.LGBMRegressor(objective='regression',num_leaves=31,learning_rate=0.05,n_estimators=20)
gbm.fit(X_train,y_train)
sc.KS_Graph(y_train, gbm.predict(X_train, num_iteration=gbm.best_iteration_))
sc.KS_Graph(y_test, gbm.predict(X_test, num_iteration=gbm.best_iteration_))



gbm.fit(X_train2,y_train2)
sc.KS_Graph(y_train2, gbm.predict(X_train2, num_iteration=gbm.best_iteration_))
sc.KS_Graph(y_test2, gbm.predict(X_test2, num_iteration=gbm.best_iteration_))










-------------------------------------
from pyhive import hive
from TCLIService.ttypes import TOperationState
import sys
import importlib
import numpy as np
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.externals.six import StringIO

%matplotlib inline
con = hive.connect(host="10.30.18.10", port=10000, username='liwenhuan',password='li@62183',auth='LDAP')
cursor = con.cursor()


spec = importlib.util.spec_from_file_location("MyFuncs","../xingchen_model_v2_20180115/MyFuncs.py")
f = importlib.util.module_from_spec(spec)
spec.loader.exec_module(f)

0.数据读取
create table policy.tmp_lwh_xiaocaizhu_fraud as select uid, max(a.group_map["contact_110"]) as contact_110, max(a.group_map["contact_120"]) ascontact_120, max(a.group_map["contact_bank"]) as contact_bank, max(a.group_map["contact_collection"]) as contact_collection, max(a.group_map["contact_court"]) as contact_court, max(a.group_map["contact_credit_card"]) as contact_credit_card, max(a.group_map["contact_each_other"]) as contact_each_other, max(a.group_map["contact_lawyer"]) as contact_lawyer, max(a.group_map["contact_loan"]) as contact_loan, max(a.group_map["contact_macao"]) as contact_macao, max(a.group_map["contact_night"]) as contact_night, max(a.group_map["dwell_used_time"]) as dwell_used_time, max(a.group_map["ebusiness_info"]) as ebusiness_info, max(a.group_map["education_info"]) as education_info, max(a.group_map["live_addr_info"]) as live_addr_info, max(a.group_map["lottery_buying"]) as lottery_buying, max(a.group_map["person_addr_changed"]) as person_addr_changed, max(a.group_map["person_ebusiness_info"]) as person_ebusiness_info, max(a.group_map["phone_call"]) as phone_call, max(a.group_map["phone_power_off"]) as phone_power_off, max(a.group_map["phone_silent"]) as phone_silent, max(a.group_map["phone_used_time"]) as phone_used_time, max(a.group_map["regular_circle"]) as regular_circle, max(a.group_map["school_addr_info"]) as school_addr_info, max(a.group_map["school_status"]) as school_status, max(a.group_map["virtual_buying"]) as virtual_buying, max(a.group_map["work_addr_info"]) as work_addr_info from( select uid, map(check_point, result) group_map from mongo.t_carrier_report_behavior_check )a

group by uid;
drop table policy.tmp_lwh_xiaocaizhu_regular; create table policy.tmp_lwh_xiaocaizhu_regular as select a., b. from ( select p_userid uid, max(a.group_map["contact_110"]) as contact_110, max(a.group_map["contact_120"]) ascontact_120, max(a.group_map["contact_bank"]) as contact_bank, max(a.group_map["contact_collection"]) as contact_collection, max(a.group_map["contact_court"]) as contact_court, max(a.group_map["contact_credit_card"]) as contact_credit_card, max(a.group_map["contact_each_other"]) as contact_each_other, max(a.group_map["contact_lawyer"]) as contact_lawyer, max(a.group_map["contact_loan"]) as contact_loan, max(a.group_map["contact_macao"]) as contact_macao, max(a.group_map["contact_night"]) as contact_night, max(a.group_map["dwell_used_time"]) as dwell_used_time, max(a.group_map["ebusiness_info"]) as ebusiness_info, max(a.group_map["education_info"]) as education_info, max(a.group_map["live_addr_info"]) as live_addr_info, max(a.group_map["lottery_buying"]) as lottery_buying, max(a.group_map["person_addr_changed"]) as person_addr_changed, max(a.group_map["person_ebusiness_info"]) as person_ebusiness_info, max(a.group_map["phone_call"]) as phone_call, max(a.group_map["phone_power_off"]) as phone_power_off, max(a.group_map["phone_silent"]) as phone_silent, max(a.group_map["phone_used_time"]) as phone_used_time, max(a.group_map["regular_circle"]) as regular_circle, max(a.group_map["school_addr_info"]) as school_addr_info, max(a.group_map["school_status"]) as school_status, max(a.group_map["virtual_buying"]) as virtual_buying, max(a.group_map["work_addr_info"]) as work_addr_info from( select p_userid, map(check_point, result) group_map from crawler.t_mobile_report_behavior_check where dt>"2019-01" )a group by p_userid )a inner join( SELECT user_id, borrow_type,add_time, success_time, STATUS, channel_category,add_channel,verifier_type,DAY,is_new, MAX(repay_period) over(partition by borrow_nid) AS repay_period, MAX(CASE WHEN STATUS IN (3,5) and DATEDIFF('2019-01-30',repay_time)>0 and borrow_nid is not null THEN 1 ELSE 0 END) over(partition by user_id) D01, MAX(CASE WHEN STATUS IN (3,5) and DATEDIFF('2019-01-30',repay_time)>0 and late_days>0 THEN 1 ELSE 0 END) over(partition by user_id) M01, row_number() over(partition by user_id order by add_time) rn FROM jujube.dw_riskdata_zc_borrow_info WHERE add_product='sxk' AND borrow_type IN (6,8) and add_time>="2019-01-01" and status in (3,5) )b on a.uid == b.user_id and b.rn=1;


df_fraud = pd.read_sql("select * from policy.tmp_lwh_xiaocaizhu_fraud a ", con=con)
df_regular = pd.read_sql("select * from policy.tmp_lwh_xiaocaizhu_regular a ", con=con)
for df in (df_fraud, df_regular):
    df.columns = df.columns.str.replace("^a\\.","")

regular_mask = (df_regular.d01==1)&(df_regular.m01==0)
df_regular[regular_mask]


for df in (df_regular, df_fraud):
    df["phone_silent_days"] = df.phone_silent.str.extract('180天内有(\d+)', expand=False)\
                              .fillna(0)\
                              .astype("float64")


for df in (df_regular, df_fraud):
    df["phone_power_off_days"] = df.phone_power_off.str.extract('关机共(\d+)天', expand=False)\
                              .fillna(0)\
                              .astype("float64")

features = ['contact_110', 'ascontact_120', 'contact_bank',
       'contact_collection', 'contact_court', 'contact_credit_card',
       'contact_each_other', 'contact_lawyer', 'contact_loan', 'contact_macao',
       'contact_night', 'dwell_used_time', 'ebusiness_info', 'education_info',
       'live_addr_info', 'lottery_buying', 'person_addr_changed',
       'person_ebusiness_info', 'phone_call', 'phone_power_off_days',
       'phone_silent_days', 'phone_used_time', 'regular_circle', 'school_addr_info',
       'school_status', 'virtual_buying', 'work_addr_info']
df_regular_red = df_regular[features].copy(deep=True)
df_fraud_red = df_fraud[features].copy(deep=True)


df_regular_red["ISBAD"]=0
df_fraud_red["ISBAD"]=1

df_regular_red.shape

model_ds = pd.concat([df_regular_red, df_fraud_red], axis=0)
model_ds.reset_index(inplace=True)
numVars = list(model_ds[features].select_dtypes(include=np.number).columns)

numDict = f.getOptimalBins(inDs=model_ds, vars=numVars, targVar="ISBAD", criterion='gini', drop_na=True)

_dict = {f:None for f in model_ds[features]}
_dict.update(numDict)

woeTabs = f.collectWoeTabs(inDs=model_ds, binDict=_dict,
                           right=True, targVar="ISBAD", createBinSeries=False)


for k,v in woeTabs.items():
    v["od_rate"] = v[1]/(v["Good&Bad"])
    display(v.style.bar(align='mid',subset=["Woe"]))


_t = woeTabs['regular_circle']
mask = _t['Good&Bad']>50
_t[mask].sort_values('od_rate', ascending=False)


from xgboost.sklearn import XGBClassifier

model_ds[features].select_dtypes(exclude=np.number).columns

model_ds_dummified = pd.get_dummies(data=model_ds[features], columns=model_ds[features].select_dtypes(exclude=np.number).columns)

trainDs_X = model_ds_dummified.sample(frac=.7, random_state= 12345)
trainDs_Y = model_ds["ISBAD"][trainDs_X.index]
testDs_X = model_ds_dummified[~model_ds_dummified.index.isin(trainDs_X.index)]
testDs_Y = model_ds["ISBAD"][~model_ds.index.isin(trainDs_X.index)]

model_ds.shape

trainDs_X.shape
trainDs_Y = model_ds["ISBAD"][trainDs_X.index]
trainDs_Y.shape
trainDs_X.index
model_ds_dummified.shape
trainDs_Y.shape
clf = XGBClassifier(max_depth=3, learning_rate=0.3, n_estimators=182,
              silent=True, booster="gbtree", random_state=1234,
              n_jobs=8, gama=0.05, min_child_weight=40, reg_alpha =1, subsample = 0.7)


clf.fit(X=trainDs_X, y=trainDs_Y, eval_metric="auc", eval_set=[(trainDs_X,trainDs_Y),(testDs_X, testDs_Y)])

node_indices = clf.apply(model_ds_dummified, ntree_limit=20)

model_ds_dummified.shape

indices_df  = pd.concat([pd.DataFrame(node_indices), model_ds["ISBAD"]], axis=1)

indices_df.shape

indices_df

model_ds_dummified.columns.values


woeTabs1 = f.collectWoeTabs(inDs=indices_df, binDict={col:None for col in indices_df.columns},
                           right=True, targVar="ISBAD", createBinSeries=False)

for k,v in woeTabs1.items():
    v["od_rate"] = v[1]/(v["Good&Bad"])
    display(v.style.bar(align='mid',subset=["Woe"]))

_estimator = clf.get_booster()

data_dot = StringIO()

clf.get_booster().dump_model(data_dot)

print(clf.get_booster().get_dump()[1])

print(data_dot.getvalue())


def xgb_node_func1(x):
    f1 = str.strip(x["phone_call"]) != '数量众多（100以上，不含100）'
    f2 = x['phone_silent_days']>=45
    if f1 and f2:
        return True
    else:
        return False

model_ds["xgboost_f1"] = model_ds.apply(xgb_node_func1, axis=1)

model_ds.shape

model_ds["xgboost_f1"].value_counts()

_t = f.getVarWoeTab(inDs=model_ds,  var="xgboost_f1", binning=None, right=True, targVar="ISBAD", createBinSeries=False)
_t["od_rate"] = _t[1]/(_t["Good&Bad"])
display(_t.style.bar(align='mid',subset=["Woe"]))
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(node_indices)
indices_one_hot = enc.transform(node_indices)
from sklearn.linear_model.logistic import LogisticRegression
logis_model = LogisticRegression()
logis_model.fit(_224, model_ds["ISBAD"])
logis_model
_224[0]
fig , ax = plt.subplots(1,1, figsize=(20,20))
xgb.plot_tree(clf, num_trees=1, ax=ax)





data kdc_user_analysis;
set kyd.zym_temp_kdc_amount_pboc_3;
m3_id_1=compress(m3_id,,'p');
appl_date=datepart(create_time);
job_province=compress(province,,'p');
aprovince=compress(job_province,"abcdefghijklmnopqrstuvwxyz");
job_city=compress(city,,'p');
acity=compress(job_city,"abcdefghijklmnopqrstuvwxyz");
incm=compress(income,,'pa');
crdsum=compress(creditsum,,'pa');
usum=compress(usedsum,,'pa');
format appl_date yymmddn8.;
if zm_score='-99' then zm_score='';
if m3_id='-9' then m3_id='';
if acity='9999' then acity='';
if aprovince='9999' then aprovince='';
aage=input(age,8.); 
gender_1=input(gender,8.);  
am3_id=input(m3_id_1,8.);
acreate_shijian=input(create_shijian,8.);
incom=input(incm,8.);
credsum=input(crdsum,8.);
uesum=input(usum,8.);
if credsum=0 then creditlv=.;
    else creditlv=(uesum/credsum);
run;



proc tabulate data=kdc_user_analysis missing order=formatted;
class gender_1;
table (all gender_1="");
title '男女比例';
run;



proc format;
value aage
.="missing"
0-22="01.18-22"
23-29="02.23-29"
30-35="03.30-35"
36-39="04.36-39"
40-46="05.40-46"
46-100="06.>=46"
;
run;


proc tabulate data=kdc_user_analysis missing order=formatted;
class aage;
table (all aage="")*N;
format aage aage.;
title '年龄分析';
run;





DROP TABLE tmp_dd_yb;
CREATE TABLE tmp_dd_yb
SELECT LEFT(add_time,7) AS TIME,add_product,borrow_type,repay_period,
IF(borrow_type=6,1,CEIL(DAY/30))AS period,
CASE WHEN is_new=1 THEN '新户' WHEN is_new=-1 THEN '次新户'
WHEN is_new=0 AND is_qx=1 THEN '迁徙户' ELSE '旧户' END AS is_new1,
CASE WHEN channel_category LIKE '%app%' THEN 'APP'
WHEN channel_category IS NULL  THEN '其他' ELSE channel_category  END AS channel,
CEIL(DATEDIFF(NOW(),repay_time)/30)AS M_diff,
CEIL(late_days/30)AS L_diff,
SUM(repay_account)AS repay_amounts,
COUNT(DISTINCT borrow_nid) AS add_num
FROM jujube.`dw_riskdata_zc_borrow_info`
WHERE add_time>='2018-09-01'
AND add_time<'2019-02-01'
AND repay_period>0
AND DATEDIFF(NOW(),repay_time)>0
AND add_product IN('shoujidai')
GROUP BY TIME,is_NEW1,channel,add_product,borrow_type,repay_period,M_diff,L_diff,period;


SELECT TIME AS 月份,
'单期' AS   产品,
'1期'AS 期数,
a.is_new1 AS 客户类型,
a.channel AS 申请渠道,
SUM(add_num)AS 首期M0笔数,
ROUND(SUM(repay_amounts)/10000,2)AS 首期M0金额,
SUM(IF(L_diff>0,add_num,0))AS 首期M1笔数,
ROUND(SUM(IF(L_diff>0,repay_amounts,0)/10000),2)AS 首期M1金额,
SUM(IF(L_diff>1,add_num,0))AS 首期M2笔数,
ROUND(SUM(IF(L_diff>1,repay_amounts,0)/10000),2)AS 首期M2金额,
SUM(IF(L_diff>2,add_num,0))AS 首期M3笔数,
ROUND(SUM(IF(L_diff>2,repay_amounts,0)/10000),2)AS 首期M3金额
FROM tmp_dd_yb a
WHERE repay_period=1
AND borrow_type=6
AND add_product='shoujidai'
AND TIME<'2019-01-01'
GROUP BY TIME,is_NEW1,channel,add_product,borrow_type
ORDER BY add_product, borrow_type,TIME,is_NEW1,channel;


SELECT TIME AS 月份,
'分期' AS 产品,
CONCAT(CEIL(period),'期')AS 期数,
a.is_new1 AS 客户类型,
a.channel AS 申请渠道,
SUM(IF(repay_period=1,add_num,0))AS 首期M0笔数,
ROUND(SUM(IF(repay_period=1,repay_amounts,0))/10000,2)AS 首期M0金额,
SUM(IF(L_diff>0 AND repay_period=1,add_num,0))AS 首期M1笔数,
ROUND(SUM(IF(L_diff>0 AND repay_period=1,repay_amounts,0))/10000,2)AS 首期M1金额,
SUM(IF(L_diff>1 AND repay_period=1,add_num,0))AS 首期M2笔数,
ROUND(SUM(IF(L_diff>1 AND repay_period=1,repay_amounts,0))/10000,2)AS 首期M2金额,
SUM(IF(L_diff>2 AND repay_period=1,add_num,0))AS 首期M3笔数,
ROUND(SUM(IF(L_diff>2 AND repay_period=1,repay_amounts,0)/10000),2)AS 首期M3金额
FROM tmp_dd_yb a
WHERE  borrow_type=8
AND TIME<'2019-01-01'
GROUP BY TIME,is_NEW1,period,channel,add_product,borrow_type,period
ORDER BY add_product, borrow_type,TIME,is_NEW1,channel,period;

/*SUM(add_num)AS 整笔M0期数,
ROUND(SUM(repay_amounts)/10000,2)AS 整笔M0金额,
SUM(IF(L_diff>0,add_num,0))AS 整笔M1期数,
ROUND(SUM(IF(L_diff>0,repay_amounts,0)/10000),2)AS 整笔M1金额,
SUM(IF(L_diff>1,add_num,0))AS 整笔M2期数,
ROUND(SUM(IF(L_diff>1,repay_amounts,0)/10000),2)AS 整笔M2金额,
SUM(IF(L_diff>2,add_num,0))AS 整笔M3期数,
ROUND(SUM(IF(L_diff>2,repay_amounts,0)/10000),2)AS 整笔M3金额
*/
DROP TABLE tmp_dd_yb;
CREATE TABLE tmp_dd_yb
SELECT LEFT(add_time,7) AS TIME,add_product,borrow_type,repay_period,
IF(borrow_type=6,1,CEIL(DAY/30))AS period,
CASE WHEN is_new=1 THEN '新户' WHEN is_new=-1 THEN '次新户'
WHEN is_new=0 AND is_qx=1 THEN '迁徙户' ELSE '旧户' END AS is_new1,
CASE WHEN channel_category LIKE '%app%' THEN 'APP'
WHEN channel_category IS NULL  THEN '其他' ELSE channel_category  END AS channel,
CASE WHEN DATEDIFF(NOW(),repay_time)>30 THEN '5'
WHEN  DATEDIFF(NOW(),repay_time)>15 THEN '4'
WHEN  DATEDIFF(NOW(),repay_time)>7 THEN '3'
WHEN  DATEDIFF(NOW(),repay_time)>3 THEN '2'
WHEN  DATEDIFF(NOW(),repay_time)>0 THEN '1'
ELSE '0'END AS M_diff,
CASE WHEN late_days>30 THEN '5'
WHEN  late_days>15 THEN '4'
WHEN  late_days>7 THEN '3'
WHEN  late_days>3 THEN '2'
WHEN  late_days>0 THEN '1'
ELSE '0' END AS L_diff,
SUM(repay_account)AS repay_amounts,
COUNT(DISTINCT borrow_nid) AS add_num
FROM jujube.`dw_riskdata_zc_borrow_info`
WHERE add_time>='2018-11-01'
AND add_time<'2019-02-06'
AND repay_period>0
AND DATEDIFF(NOW(),repay_time)>0
AND add_product IN('sxk')
GROUP BY TIME,is_NEW1,channel,add_product,borrow_type,repay_period,M_diff,L_diff,period;


SELECT TIME AS 月份,
'神下款' AS   产品,
a.is_new1 AS 客户类型,
a.channel AS 申请渠道,
SUM(add_num)AS M0笔数,
ROUND(SUM(repay_amounts)/10000,2)AS M0金额,
SUM(IF(L_diff>0,add_num,0))AS  0笔数,
ROUND(SUM(IF(L_diff>0,repay_amounts,0)/10000),2)AS 0金额,
SUM(IF(L_diff>1,add_num,0))AS 3笔数,
ROUND(SUM(IF(L_diff>1,repay_amounts,0)/10000),2)AS 3金额,
SUM(IF(L_diff>2,add_num,0))AS 7笔数,
ROUND(SUM(IF(L_diff>2,repay_amounts,0)/10000),2)AS 7金额,
SUM(IF(L_diff>3,add_num,0))AS 15笔数,
ROUND(SUM(IF(L_diff>3,repay_amounts,0)/10000),2)AS 15金额,
SUM(IF(L_diff>4,add_num,0))AS 30笔数,
ROUND(SUM(IF(L_diff>4,repay_amounts,0)/10000),2)AS 30金额
FROM tmp_dd_yb a
WHERE repay_period=1
AND add_product='sxk'
GROUP BY TIME,is_NEW1,channel,add_product,borrow_type
ORDER BY add_product, borrow_type,TIME,is_NEW1,channel;


  -- 逾期整体情况
DROP TABLE policy.tmp_zym_big_repay_1;
CREATE TABLE policy.tmp_zym_big_repay_1 AS 
SELECT a.*,
        
        case when day>=1 and late_days>0 then 1 else 0  end as one,
        case when day>=3 and late_days>0 then 1 else 0  end as three,
        case when day>=10 and late_days>0 then 1 else 0  end as ten,
        case when day>=30 and late_days>0 then 1 else 0  end as thirty,
            
        case when late_days<=1 and repay_status='LATE_END' then 1 else 0  end as one_end,
        case when late_days<=3 and repay_status='LATE_END' then 1 else 0  end as three_end,
        case when late_days<=10  and repay_status='LATE_END'then 1 else 0  end as ten_end,
        case when late_days<=30  and repay_status='LATE_END' then 1 else 0  end as thirty_end


FROM  policy.tmp_zym_big_repay a;


--根据标识做统计
DROP TABLE policy.tmp_zym_big_repay_2;
CREATE TABLE policy.tmp_zym_big_repay_2 AS 
SELECT substring(add_time,1,7) AS DATE,customer_cluster_type,repay_period,
COUNT(borrow_nid) AS daoqi,
sum(if(one=1,1,0))  as one_daoqi,
sum(if(three=1,1,0))  as three_daoqi,
sum(if(ten=1,1,0))  as ten_daoqi,
sum(if(thirty=1,1,0))  as thirty_daoqi,

ROUND(SUM(repay_capital)) AS daoqi_captical,
sum(if(one=1,repay_capital,0))  as one_daoqi_capital,
sum(if(three=1,repay_capital,0))  as three_daoqi_capital,
sum(if(ten=1,repay_capital,0))  as ten_daoqi_capital,
sum(if(thirty=1,repay_capital,0))  as thirty_daoqi_capital,

SUM(IF(late_days>0,1,0)) AS rucui,
sum(if(one=1 and one_end=1,1,0))  as one_end_hui,
sum(if(three=1 and three_end=1,1,0))  as three_end_hui,
sum(if(ten=1 and ten_end=1,1,0))  as ten_end_hui,
sum(if(thirty=1 and thirty_end=1,1,0))  as thirty_end_hui,

ROUND(SUM(IF(late_days>0,repay_capital,0))) AS rucui_captical,
sum(if(one=1 and one_end=1,repay_capital,0))  as one_end_hui_capital,
sum(if(three=1 and three_end=1,repay_capital,0))  as three_end_hui_capital,
sum(if(ten=1 and ten_end=1,repay_capital,0))  as ten_end_hui_capital,
sum(if(thirty=1 and thirty_end=1,repay_capital,0))  as thirty_end_hui_capital,


sum(if(one=1 and one_end=1,1,0))/sum(if(one=1,1,0)) as one_rate,
sum(if(three=1 and three_end=1,1,0))/sum(if(three=1,1,0)) as three_rate,
sum(if(ten=1 and ten_end=1,1,0))/sum(if(ten=1,1,0)) as ten_rate,
sum(if(thirty=1 and thirty_end=1,1,0))/sum(if(thirty=1,1,0)) as thirty_rate,

sum(if(one=1 and one_end=1,repay_capital,0))/sum(if(one=1,repay_capital,0)) as one_rate_c,
sum(if(three=1 and three_end=1,repay_capital,0))/sum(if(three=1,repay_capital,0)) as three_rate_c,
sum(if(ten=1 and ten_end=1,repay_capital,0))/sum(if(ten=1,repay_capital,0)) as ten_rate_c,
sum(if(thirty=1 and thirty_end=1=1,repay_capital,0))/sum(if(thirty=1,repay_capital,0)) as thirty_rate_c


from policy.tmp_zym_big_repay_1
where repay_time<'2018-10-22'
group by substring(add_time,1,7),customer_cluster_type,repay_period;


select * from  policy.tmp_zym_big_repay_2;


SET @star_time='2018-04-01 00:00:00',
    @end_time='2018-10-01 00:00:00';
    
DROP TABLE IF EXISTS policy.tmp_zym_sjd_loan;
CREATE TABLE policy.tmp_zym_sjd_loan
SELECT user_id,borrow_nid,add_time,account,
       CASE WHEN borrow_type IN (6,7) THEN '01.dq'
            WHEN borrow_type=8 THEN '02.fq'
       ELSE '03.qt' END AS product_type,
       CASE WHEN is_new IN (-1,1) THEN '01.new'
            WHEN is_new=0 AND is_qx=0 THEN '02.old'
            WHEN is_new=0 AND is_qx=1 THEN '03.qx'
       ELSE '04.qt' END AS kh_bin,
       CASE WHEN DAY=90 THEN 1
            WHEN DAY=180 THEN 2
            WHEN DAY=270 THEN 3
            WHEN DAY=360 THEN 4
       ELSE 5 END AS fq_type,
       CASE WHEN channel_category LIKE 'app%' THEN '01.app'
            WHEN channel_category='流量超市' THEN '02.market'
            WHEN channel_category='微站' THEN '03.wechat'
       ELSE '04.qt' END AS channel_bin
FROM jujube.`dw_riskdata_zc_borrow_info`
WHERE borrow_type IN (6,7,8)
  AND STATUS IN (3,5)
  AND add_product='shoujidai'
  AND repay_period=1
  AND add_time>=@star_time
  AND add_time<@end_time
;
CREATE INDEX index1 ON policy.tmp_zym_sjd_loan(borrow_nid);


SET @date='2018-10-01';
DROP TABLE IF EXISTS policy.tmp_zym_sjd_end_over;
CREATE TABLE IF NOT EXISTS policy.tmp_zym_sjd_end_over AS
SELECT 	DATE_FORMAT(B.add_time,'%Y-%m') AS loan_month,
	A.BORROW_NID,
	A.ID AS schedule_id,
	B.USER_ID,
	B.fq_type,
	B.product_type,
	B.kh_bin,
	B.channel_bin,
	A.REPAY_CAPITAL AS amount,
	0 AS repay_amount,
	A.repay_time,
	A.REPAY_YESTIME,
	CASE WHEN A.repay_yestime IS NULL AND A.REPAY_TIME>=@DATE THEN 0
             WHEN A.repay_yestime IS NULL AND A.REPAY_TIME<@DATE THEN DATEDIFF(DATE_SUB(@DATE,INTERVAL 1 DAY),A.repay_time)
             WHEN A.repay_yestime IS NOT NULL AND A.REPAY_TIME>=@DATE THEN 0
             WHEN A.repay_yestime IS NOT NULL AND A.REPAY_TIME<@DATE AND A.repay_yestime>=@date AND A.repay_yestime>A.REPAY_TIME THEN DATEDIFF(DATE_SUB(@DATE,INTERVAL 1 DAY),A.repay_time)
             WHEN A.repay_yestime IS NOT NULL AND A.REPAY_TIME<@DATE AND A.repay_yestime<@date AND A.repay_yestime>A.REPAY_TIME THEN DATEDIFF(A.repay_yestime,A.repay_time)
             WHEN A.repay_yestime IS NOT NULL AND A.REPAY_TIME<@DATE AND A.repay_yestime<=A.REPAY_TIME THEN 0      
	END AS late_days,
	0 AS max_late_days
FROM 	cw_mobp2p.t_repay_schedule AS A INNER JOIN 
	policy.tmp_zym_sjd_loan AS B ON A.BORROW_NID=B.BORROW_NID
WHERE 	B.add_time<@date
;
CREATE INDEX index1 ON policy.tmp_zym_sjd_end_over(borrow_nid);
CREATE INDEX index2 ON policy.tmp_zym_sjd_end_over(schedule_id);


-- 计算月底最大逾期天数
DROP TABLE policy.tmp_zym_max_late_day;
CREATE TABLE policy.tmp_zym_max_late_day
SELECT borrow_nid,
       MAX(late_days) AS max_late_days
FROM policy.tmp_zym_sjd_end_over
GROUP BY 1;
ALTER TABLE policy.tmp_zym_max_late_day ADD INDEX(borrow_nid);
UPDATE  policy.tmp_zym_sjd_end_over AS A INNER JOIN
	policy.tmp_zym_max_late_day AS B ON A.borrow_nid=B.borrow_nid
SET	A.max_late_days=B.max_late_days;
-- 计算月底已还金额

SET @date='2018-10-01';
DROP TABLE IF EXISTS policy.tmp_zym_repay_0;
CREATE TABLE policy.tmp_zym_repay_0 AS
	SELECT a.*,
		B.AMOUNT AS repay_amount_0
	FROM 	policy.tmp_zym_sjd_end_over a 
		INNER JOIN cw_mobp2p.t_repay_write_off  B ON a.schedule_id=B.SCHEDULE_ID 
		INNER JOIN cw_mobp2p.t_account_repay  C ON B.REPAY_INFO_ID=C.ID
	WHERE 	B.STATUS IN ('SUCCESS','FINISHED')
		AND 	B.TYPE='REPAY'/*正常还款:REPAY*/
		AND 	B.DELETED_FLAG=1
		AND 	C.STATUS IN ('FINISHED','FINISHED_PENDING','FINISHED_REFUND','PENDING')/*  挂账中:PENDING  已销账:FINISHED*/
		AND 	C.REPAY_TIME<@date
	;
CREATE INDEX index2 ON policy.tmp_zym_repay_0(schedule_id);


DROP TABLE IF EXISTS policy.tmp_zym_repay;
CREATE TABLE policy.tmp_zym_repay AS
	SELECT schedule_id,
		SUM(repay_amount_0) AS repay_amount
	FROM policy.tmp_zym_repay_0
	GROUP BY schedule_id;
-- 1
ALTER TABLE policy.tmp_zym_repay ADD INDEX(schedule_id);


-- 修改已还金额
UPDATE  policy.tmp_zym_sjd_end_over AS A INNER JOIN
	policy.tmp_zym_repay AS B ON A.schedule_id=B.schedule_id
SET	A.repay_amount=B.repay_amount;
-- 计算月底逾期
SET @date='2018-10-01';
SELECT	DATE_FORMAT(DATE_SUB(@date,INTERVAL 1 DAY),'%Y-%m') AS '截止月份',
        fq_type,
	product_type,
	kh_bin,
	channel_bin,
	loan_month,
	COUNT(DISTINCT BORROW_NID) AS cnt,
	SUM(amount) AS amt,
	SUM(CASE WHEN max_late_days>90 THEN amount-repay_amount ELSE 0 END) AS m4_wh_total,
	COUNT(DISTINCT CASE WHEN max_late_days>90 THEN BORROW_NID END) AS m4_cnt
FROM 	policy.tmp_zym_sjd_end_over
GROUP BY 1,2,3,4,5,6


--xinyan
import pandas as pd
import numpy as np
import importlib

from matplotlib import pyplot as plt
from xgboost import plot_importance

import xgboost

spec = importlib.util.spec_from_file_location("MyFuncs","../xingchen_model_v2_20180115/MyFuncs.py")
f = importlib.util.module_from_spec(spec)
spec.loader.exec_module(f)

pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 500)
pd.set_option('precision',4)

%matplotlib inline

0.数据读取
df = pd.read_excel("./xinyan_validate.xlsx")
df.columns
Index(['borrow_nid', 'user_id', 'borrow_type', 'add_time', 'success_time',
       'channel_category', 'add_channel', 'verifier_type', 'DAY', 'is_new',
       'repay_period', 'D10', 'M10', 'D10_account', 'M10_account',
       'borrow_nid_1', 'acc_exc', 'acc_sleep', 'apply_credibility',
       'apply_score', 'consfin_org_count', 'currently_overdue',
       'currently_performance', 'history_fail_fee', 'history_suc_fee',
       'latest_one_month', 'latest_one_month_fail', 'latest_one_month_suc',
       'latest_overdue_time', 'latest_query_time', 'latest_six_month',
       'latest_three_month', 'loans_cash_count', 'loans_count',
       'loans_credibility', 'loans_latest_time', 'loans_long_time',
       'loans_org_count', 'loans_overdue_count', 'loans_score',
       'loans_settle_count', 'max_overdue_amt', 'max_overdue_days',
       'query_cash_count', 'query_finance_count', 'query_org_count',
       'query_sum_count', 'xy_140001', 'xy_140002', 'xy_140003', 'xy_140004',
       'xy_140005', 'xy_140006', 'xy_140007', 'xy_140008', 'xy_140009',
       'xy_140010', 'xy_140011', 'xy_140012', 'xy_140013', 'xy_140014',
       'xy_140015', 'xy_140016', 'xy_140017', 'xy_140018', 'xy_140019',
       'xy_140020', 'xy_140021', 'xy_140022', 'xy_140023', 'xy_140024',
       'xy_140025', 'xy_140026', 'xy_140027', 'xy_140028', 'xy_140029',
       'xy_140030', 'xy_140031'],
      dtype='object')
features  = ['acc_exc', 'acc_sleep', 'apply_credibility',
       'apply_score', 'consfin_org_count', 'currently_overdue',
       'currently_performance', 'history_fail_fee', 'history_suc_fee',
       'latest_one_month', 'latest_one_month_fail', 'latest_one_month_suc',
       'latest_overdue_time', 'latest_query_time', 'latest_six_month',
       'latest_three_month', 'loans_cash_count', 'loans_count',
       'loans_credibility', 'loans_latest_time', 'loans_long_time',
       'loans_org_count', 'loans_overdue_count', 'loans_score',
       'loans_settle_count', 'max_overdue_amt', 'max_overdue_days',
       'query_cash_count', 'query_finance_count', 'query_org_count',
       'query_sum_count', 'xy_140001', 'xy_140002', 'xy_140003', 'xy_140004',
       'xy_140005', 'xy_140006', 'xy_140007', 'xy_140008', 'xy_140009',
       'xy_140010', 'xy_140011', 'xy_140012', 'xy_140013', 'xy_140014',
       'xy_140015', 'xy_140016', 'xy_140017', 'xy_140018', 'xy_140019',
       'xy_140020', 'xy_140021', 'xy_140022', 'xy_140023', 'xy_140024',
       'xy_140025', 'xy_140026', 'xy_140027', 'xy_140028', 'xy_140029',
       'xy_140030', 'xy_140031']
1. 数据清洗
# 时间类型变量，和贷款时间做差，形成新变量
s1 = df["loans_latest_time"].apply(pd.Timestamp)
s2 = df.success_time.apply(pd.Timestamp)
df["success_to_lastest_loan"] = (s1 - s2).dt.days
s1 = df.latest_query_time.apply(pd.Timestamp)
s2 = df.success_time.apply(pd.Timestamp)
df["success_to_qry"] = (s1 - s2).dt.days
#drop原有的变量
features.remove("loans_latest_time") 
features.remove("latest_query_time")
#添加新的变量
features = features + ["success_to_lastest_loan","success_to_qry"]
2 数据质量
2.1 数值型变量
stats_tab = df[features].describe().T
display(stats_tab)



_todrop = stats_tab[stats_tab["min"] == stats_tab["max"]]
_todrop_vars =_todrop.index.values
display(_todrop)


2.2 非数值型变量
stats_tab = df[features].select_dtypes(include=np.object).describe().T
_todrop_vars = np.append(_todrop_vars, stats_tab[stats_tab["count"]==0].index)
display(stats_tab)


3 逾期情况
已经定义了M10为逾期，dpd10+，所有用户皆为分期
df["M10"].value_counts().plot.pie(legend=True)


df["M10"].value_counts()/df["M10"].size

df["M10"].mean()


4 特征预测能力分析
4.1 变量分箱
# 数值变量按10分位数切分
num_features = df[features].select_dtypes(include=[np.number])
binAutoDict = f.getAutoQuanDict(inDs=df, varList=num_features, binQ= np.arange(0,1.1,0.1))
# 非数值 
non_num_bin_dict = { col:None for col in df[features].select_dtypes(exclude=[np.number]).columns }
binAutoDict.update(non_num_bin_dict)
 woeTabs = f.collectWoeTabs(inDs= df, binDict=binAutoDict,targVar="M10")




4.2变量预测能力探索

# 存在单区间高于整体两倍的变量，分段想起
iv_map = {}
for k,v in woeTabs.items():
    try:
        v['od_rate'] = v[1]/(v["Good&Bad"])
        iv_map.update({k: v["Iv_sum"].iloc[0]})
        if v["od_rate"].max()>df["M10"].mean()*2:
            display(v.style.bar(align='mid',subset=["Woe"]))
            print("\n")
    except IndexError as e:
        print(e)

4.2.1 IV汇总

pd.Series(iv_map).sort_values(ascending=False)
pd.Series(iv_map).sort_values(ascending=True).plot(kind="barh",figsize=(20,20))

iv_map = {}
for k,v in woeTabs.items():
    try:
        v['od_rate'] = v[1]/(v["Good&Bad"])
        iv_map.update({k: v["Iv_sum"].iloc[0]})
        display(v.style.bar(align='mid',subset=["Woe"]))
        print("\n")
    except IndexError as e:
        print(e)



# str_obj_columns
cat_columns = ["latest_overdue_time", "max_overdue_amt", "max_overdue_days", "xy_140027", "xy_140028"]
_features_droped = [col for col in features if col not in cat_columns]
transed_df = pd.get_dummies(df[cat_columns])
transed_df.columns = transed_df.columns.str.replace("\\[|\\]", "_")
full_df = pd.concat([df, transed_df], axis=1)
feat_w_dummies =  _features_droped + list(transed_df.columns)
full_df.columns = full_df.columns.str.replace("\\[|\\]", "_")
trainDs_X = full_df.sample(frac=.7, random_state= 12345)[feat_w_dummies]
trainDs_Y = full_df["M10"][trainDs_X.index]
testDs_X = full_df[~df.index.isin(trainDs_X.index)][feat_w_dummies]
testDs_Y = full_df["M10"][~df.index.isin(trainDs_X.index)]
param = {'max_depth': 2, 'eta': 0.2, 'verbosity': 1, 'objective': 'binary:logistic', 'lambda':1 , }
param['nthread'] = 4
param['eval_metric'] = 'auc'
train = xgb.DMatrix(trainDs_X, label = trainDs_Y)
test = xgb.DMatrix(testDs_X, label = testDs_Y)
evallist = [(test, 'eval'), (train, 'train')]
num_round = 300
bst = xgb.train(param, train, num_round, evallist)

def my_plot_importance(booster, figsize, **kwargs): 
    from matplotlib import pyplot as plt
    from xgboost import plot_importance
    fig, ax = plt.subplots(1,1,figsize=figsize)
    return plot_importance(booster=booster, ax=ax, **kwargs)
my_plot_importance(booster=bst,figsize=(10,10), height=0.8)

fig , ax = plt.subplots(1,1, figsize=(20,20))
xgb.plot_tree(bst, num_trees=3, ax=ax)
